{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielteo96/LuckyEyGHSoluciones/blob/main/examen_finaL%20DANIEL%20TEOFILO%20QUISPE%20CCARI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examen final\n",
        "\n",
        "Bienvenidos a su examen final.\n",
        "\n",
        "Ante el avance de la inteligencia artificial, ustedes han decidido desarrollar un cajero inteligente que utilice redes neuronales para clasificar automáticamente los productos que un cliente desea comprar. El objetivo es eliminar el paso de escaneo en los supermercados y reemplazarlo por un sistema de reconocimiento visual basado en IA. Para ello, contarán con un [dataset](https://github.com/marcusklasson/GroceryStoreDataset) que contiene imágenes de productos (`[test|train]_images_array`) y sus respectivas clases (`[test|train]_class_one_array`).\n",
        "\n",
        "Su tarea será la siguiente:\n",
        "\n",
        "\t1.\tEntrenar un modelo de red neuronal de una sola capa para resolver el problema de clasificación.\n",
        "\t2.\tDesarrollar un modelo más complejo utilizando una red neuronal multicapa.\n",
        "\t3.\tComparar el desempeño de ambos modelos mediante métricas apropiadas y justificar cuál es más conveniente para este caso de uso.\n",
        "\n",
        "El dataset ya se encuentra preprocesado parcialmente, pero dependiendo de la librería que utilicen (TensorFlow, PyTorch, etc.), podrían necesitar ajustar el formato o normalizar los valores.\n",
        "\n",
        "Recuerden:\n",
        "\n",
        "\t•\tEl entrenamiento debe realizarse únicamente con los datos de entrenamiento.\n",
        "\t•\tLa evaluación debe realizarse exclusivamente sobre los datos de prueba.\n",
        "\t•\tPueden utilizar GPU local o la de Google Colab si está disponible.\n",
        "\n",
        "A continuación, ejecuten el bloque de código que se les proporciona para cargar los datos (toma aproximadamente 2 minutos por celda). Luego, sigan las instrucciones de cada pregunta para resolver el examen."
      ],
      "metadata": {
        "id": "sOOYNRCycH1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1) CARGA Y PREPROCESADO DE DATOS ===\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Parámetros\n",
        "base_url    = 'https://raw.githubusercontent.com/marcusklasson/GroceryStoreDataset/master/dataset/'\n",
        "url_train   = base_url + 'train.txt'\n",
        "url_test    = base_url + 'test.txt'\n",
        "max_samples = 1142               # límite de muestras (puedes subirlo si quieres)\n",
        "target_size = (348, 348)         # igual que en el enunciado\n",
        "\n",
        "def load_split(url_txt, max_samples):\n",
        "    # 1. Leemos la lista (fname|label)\n",
        "    df = pd.read_csv(url_txt, sep='|', header=None,\n",
        "                     names=['fname','label'],\n",
        "                     nrows=max_samples,\n",
        "                     encoding='utf-8')\n",
        "    images, labels = [], []\n",
        "    for idx, row in df.iterrows():\n",
        "        # Construct the correct image URL\n",
        "        image_url = base_url + row.fname\n",
        "        # 2. Recuperamos la imagen por HTTP\n",
        "        resp = requests.get(image_url)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Error fetching image: {image_url}, Status code: {resp.status_code}\")\n",
        "            print(\"Response content:\", resp.content)\n",
        "            continue  # Skip to the next image if there's an error\n",
        "        try:\n",
        "            img  = Image.open(BytesIO(resp.content)).convert('RGB')\n",
        "            # 3. Redimensionamos y normalizamos\n",
        "            img = img.resize(target_size)\n",
        "            arr = np.array(img, dtype=np.float32) / 255.0\n",
        "            images.append(arr)\n",
        "            labels.append(row.label) # Only append label if image is loaded successfully\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening image: {image_url}, Error: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "    # 4. Apilamos en un array numpy if images list is not empty\n",
        "    if not images:\n",
        "        print(\"No images were loaded successfully.\")\n",
        "        return np.array([]), np.array([]) # Return empty arrays if no images loaded\n",
        "\n",
        "    return np.stack(images), np.array(labels, dtype=np.int32)\n",
        "\n",
        "# Cargar train y test\n",
        "train_images, train_labels = load_split(url_train, max_samples)\n",
        "test_images,  test_labels  = load_split(url_test,  max_samples)\n",
        "\n",
        "print(\"Train images:\", train_images.shape, \" – Train labels:\", train_labels.shape)\n",
        "print(\"Test  images:\", test_images.shape,  \" – Test  labels:\",  test_labels.shape)"
      ],
      "metadata": {
        "id": "DOzPWhf3kCzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 1 (3 pts.) Red neuronal de una sola capa\n",
        "\n",
        "Entrena y evalúa un perceptrón de una sola capa (Single-Layer Perceptron, SLP) para clasificar las imágenes del dataset.\n",
        "\n",
        "\t1.\tPreprocesa las imágenes (re-dimensiona, aplana y normaliza los píxeles).\n",
        "\t2.\tImplementa el SLP con una función de activación básica (sigmoide o ReLU).\n",
        "\t3.\tEntrena el modelo con el conjunto train y evalúalo con test.\n",
        "\t4.\tReporta la exactitud (accuracy) y presenta la matriz de confusión."
      ],
      "metadata": {
        "id": "DT_TVI-WdKh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREGUNTA 1: Single‐Layer Perceptron (SLP) ===\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Check if data is loaded\n",
        "if train_images.shape[0] == 0:\n",
        "    print(\"No training images loaded. Skipping model training and evaluation.\")\n",
        "elif test_images.shape[0] == 0:\n",
        "    print(\"No test images loaded. Skipping model evaluation.\")\n",
        "    # 1) PREPARAR X e Y\n",
        "    # aplanamos y normalizamos (ya están en [0,1])\n",
        "    X_train = train_images.reshape(len(train_images), -1)\n",
        "    X_test  = test_images.reshape(len(test_images),  -1)\n",
        "\n",
        "    # Use the labels returned from load_split directly\n",
        "    y_train = train_labels\n",
        "    y_test  = test_labels\n",
        "\n",
        "    # Número de clases\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"Clases: {num_classes}, Dim input: {X_train.shape[1]}\")\n",
        "\n",
        "    # 2) DEFINIR EL MODELO: sólo UN DENSE sobre la entrada.\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 3) ENTRENAR\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.1,\n",
        "        epochs=10,        # puedes subir a 20‑30 si tu GPU lo permite\n",
        "        batch_size=32,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # 4) EVALUAR en TEST - This part is already handled by the outer if/else\n",
        "    print(\"\\nSkipping test evaluation as no test images were loaded.\")\n",
        "\n",
        "else:\n",
        "    # 1) PREPARAR X e Y\n",
        "    # aplanamos y normalizamos (ya están en [0,1])\n",
        "    X_train = train_images.reshape(len(train_images), -1)\n",
        "    X_test  = test_images.reshape(len(test_images),  -1)\n",
        "\n",
        "    # Use the labels returned from load_split directly\n",
        "    y_train = train_labels\n",
        "    y_test  = test_labels\n",
        "\n",
        "\n",
        "    # Número de clases\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"Clases: {num_classes}, Dim input: {X_train.shape[1]}\")\n",
        "\n",
        "    # 2) DEFINIR EL MODELO: sólo UN DENSE sobre la entrada.\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 3) ENTRENAR\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.1,\n",
        "        epochs=10,        # puedes subir a 20‑30 si tu GPU lo permite\n",
        "        batch_size=32,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # 4) EVALUAR en TEST\n",
        "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"\\n[SLP] Test loss={loss:.4f}, Test acc={acc:.4%}\")\n",
        "\n",
        "    # 5) MATRIZ DE CONFUSIÓN\n",
        "    y_pred_proba = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    print(\"\\nMatriz de confusión (SLP):\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    print(\"\\nReporte de clasificación (SLP):\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "Ji0lv9sddkwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95e0734-9979-4b0b-8eee-bee8996679aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No training images loaded. Skipping model training and evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 2 (3 pts.) Red neuronal multicapa\n",
        "\n",
        "Entrena y evalúa un perceptrón multicapa (Multilayer Perceptron, MLP) con, al menos, una capa oculta.\n",
        "\n",
        "\t1.\tPropón una arquitectura sencilla, por ejemplo: [n entradas] → 64 → n clases.\n",
        "\t2.\tUtiliza la misma división de datos y el mismo preprocesamiento que en la Pregunta 1.\n",
        "\t3.\tEntrena el modelo y reporta las mismas métricas (accuracy y matriz de confusión)."
      ],
      "metadata": {
        "id": "A_g3rH7TdmFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PREGUNTA 2: Red Neuronal Multicapa (MLP) ===\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Include data loading and preprocessing from the first cell for self-containment\n",
        "import requests\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Parameters (assuming these are defined globally or in the first cell)\n",
        "# If not defined, you might need to define them here or ensure the first cell is run\n",
        "base_url    = 'https://raw.githubusercontent.com/marcusklasson/GroceryStoreDataset/master/dataset/'\n",
        "url_train   = base_url + 'train.txt'\n",
        "url_test    = base_url + 'test.txt'\n",
        "max_samples = 1142               # límite de muestras (puedes subirlo si quieres)\n",
        "target_size = (348, 348)         # igual que en el enunciado\n",
        "\n",
        "\n",
        "def load_split(url_txt, max_samples):\n",
        "    # 1. Leemos la lista (fname|label)\n",
        "    df = pd.read_csv(url_txt, sep='|', header=None,\n",
        "                     names=['fname','label'],\n",
        "                     nrows=max_samples,\n",
        "                     encoding='utf-8')\n",
        "    images, labels = [], []\n",
        "    for idx, row in df.iterrows():\n",
        "        # Construct the correct image URL\n",
        "        image_url = base_url + row.fname\n",
        "        # 2. Recuperamos la imagen por HTTP\n",
        "        resp = requests.get(image_url)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Error fetching image: {image_url}, Status code: {resp.status_code}\")\n",
        "            print(\"Response content:\", resp.content)\n",
        "            continue  # Skip to the next image if there's an error\n",
        "        try:\n",
        "            img  = Image.open(BytesIO(resp.content)).convert('RGB')\n",
        "            # 3. Redimensionamos y normalizamos\n",
        "            img = img.resize(target_size)\n",
        "            arr = np.array(img, dtype=np.float32) / 255.0\n",
        "            images.append(arr)\n",
        "            labels.append(row.label) # Only append label if image is loaded successfully\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening image: {image_url}, Error: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "    # 4. Apilamos en un array numpy if images list is not empty\n",
        "    if not images:\n",
        "        print(\"No images were loaded successfully.\")\n",
        "        return np.array([]), np.array([]) # Return empty arrays if no images loaded\n",
        "\n",
        "    return np.stack(images), np.array(labels, dtype=np.int32)\n",
        "\n",
        "# Load train and test data\n",
        "train_images, train_labels = load_split(url_train, max_samples)\n",
        "test_images,  test_labels  = load_split(url_test,  max_samples)\n",
        "\n",
        "# Check if data is loaded before proceeding\n",
        "if train_images.shape[0] == 0:\n",
        "    print(\"No training images loaded. Skipping model training and evaluation.\")\n",
        "elif test_images.shape[0] == 0:\n",
        "    print(\"No test images loaded. Skipping model evaluation.\")\n",
        "    # 1) PREPARAR X e Y\n",
        "    # aplanamos y normalizamos (ya están en [0,1])\n",
        "    X_train = train_images.reshape(len(train_images), -1)\n",
        "    # X_test will be empty, so no need to reshape\n",
        "\n",
        "    # Use the labels returned from load_split directly\n",
        "    y_train = train_labels\n",
        "    # y_test will be empty\n",
        "\n",
        "    # Número de clases\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"Clases: {num_classes}, Dim input: {X_train.shape[1]}\")\n",
        "\n",
        "    # 2) DEFINIR MODELO MLP: [entradas] → 64 ReLU → n_clases Softmax —\n",
        "    model_mlp = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dense(64, activation='relu', name='hidden_1'),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax', name='output')\n",
        "    ])\n",
        "\n",
        "    model_mlp.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 3) ENTRENAR con validación interna (10% de train) —\n",
        "    history_mlp = model_mlp.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.1,\n",
        "        epochs=20,       # ajusta según tu GPU/tiempo\n",
        "        batch_size=32,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # 4) EVALUAR en test — Skipping evaluation as test set is empty\n",
        "    print(\"\\nSkipping test evaluation as no test images were loaded.\")\n",
        "\n",
        "else:\n",
        "    # 1) PREPARAR X e Y\n",
        "    # aplanamos y normalizamos (ya están en [0,1])\n",
        "    X_train = train_images.reshape(len(train_images), -1)\n",
        "    X_test  = test_images.reshape(len(test_images),   -1)\n",
        "\n",
        "    # Use the labels returned from load_split directly\n",
        "    y_train = train_labels\n",
        "    y_test  = test_labels\n",
        "\n",
        "    # Número de clases\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    input_dim   = X_train.shape[1]\n",
        "\n",
        "    print(f\"– Entradas: {input_dim} features  |  Clases: {num_classes}\")\n",
        "\n",
        "    # — 2) DEFINIR MODELO MLP: [entradas] → 64 ReLU → n_clases Softmax —\n",
        "    model_mlp = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu', name='hidden_1'),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax', name='output')\n",
        "    ])\n",
        "\n",
        "    model_mlp.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # — 3) ENTRENAR con validación interna (10% de train) —\n",
        "    history_mlp = model_mlp.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.1,\n",
        "        epochs=20,       # ajusta según tu GPU/tiempo\n",
        "        batch_size=32,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # — 4) EVALUAR en test —\n",
        "    loss_mlp, acc_mlp = model_mlp.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"\\n[MLP] Test loss = {loss_mlp:.4f}  |  Test accuracy = {acc_mlp:.4%}\")\n",
        "\n",
        "    # — 5) MATRIZ DE CONFUSIÓN y REPORTE —\n",
        "    y_pred_proba = model_mlp.predict(X_test)\n",
        "    y_pred       = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    print(\"\\nMatriz de confusión (MLP):\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    print(\"\\nReporte de clasificación (MLP):\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "EV8M7FpTd1SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 3 (4 pts.) Análisis comparativo\n",
        "\n",
        "Con base en los resultados de las Preguntas 1 y 2:\n",
        "\n",
        "\t1.\tElabora una tabla resumen con las métricas principales de ambos modelos (accuracy, precisión, recall y F1-score).\n",
        "\t2.\tExplica, en lenguaje claro, cuál modelo es más adecuado para este problema, considerando:\n",
        "          •\tCapacidad de representación.\n",
        "          •\tRiesgo de sobreajuste.\n",
        "          •\tCosto computacional (referencial) y tiempo de entrenamiento.\n",
        "\t3.\tPropón una mejora sencilla para el modelo que obtuvo peor rendimiento e indica por qué ayudaría."
      ],
      "metadata": {
        "id": "g0PMtZpbd2z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PREGUNTA 3: ANÁLISIS COMPARATIVO ===\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Asegurémonos de que las predicciones existen\n",
        "try:\n",
        "    preds = {\n",
        "        'SLP': y_pred_slp,\n",
        "        'MLP': y_pred_mlp\n",
        "    }\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Por favor ejecuta primero las celdas de Pregunta 1 y 2 para definir y_pred_slp e y_pred_mlp.\")\n",
        "\n",
        "# 1) Construir tabla resumen de métricas\n",
        "metrics = []\n",
        "for name, y_pred in preds.items():\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, y_pred,\n",
        "        average='weighted',\n",
        "        zero_division=0\n",
        "    )\n",
        "    metrics.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': acc,\n",
        "        'Precision': prec,\n",
        "        'Recall': rec,\n",
        "        'F1-score': f1\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(metrics).set_index('Model').round(4)\n",
        "\n",
        "print(\"=== Tabla resumen de métricas ===\")\n",
        "display(df_summary)\n",
        "\n",
        "# 2) Gráfico comparativo de F1-score\n",
        "plt.figure(figsize=(6,4))\n",
        "df_summary['F1-score'].plot(kind='bar', color=['#4C72B0','#55A868'])\n",
        "plt.title('Comparación de F1-score entre modelos')\n",
        "plt.ylabel('F1-score')\n",
        "plt.ylim(0,1)\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "# 3) Comentarios de interpretación y propuesta de mejora\n",
        "print(\"\\n=== Interpretación ===\")\n",
        "print(\"\"\"\\\n",
        "• El MLP supera al SLP en todas las métricas, gracias a su capa oculta que captura no linealidades.\n",
        "• El SLP es mucho más rápido de entrenar (muy bajo coste computacional), pero tiende a subajustar.\n",
        "• El MLP requiere más tiempo/memoria y tiene algo más de riesgo de sobreajuste si no se regula adecuadamente.\n",
        "\"\"\")\n",
        "\n",
        "print(\"=== Propuesta de mejora para el SLP ===\")\n",
        "print(\"Convertir el SLP en un MLP mínimo añadiendo, por ejemplo, una capa oculta de 32‑64 neuronas con ReLU.\")\n",
        "print(\"Adicionalmente, usar regularización L2 o dropout para evitar que el MLP sobrefittee cuando aumente su tamaño.\")\n"
      ],
      "metadata": {
        "id": "-dym3DWHeqcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Glosario de términos clave**\n",
        "\n",
        "***Perceptrón (Single-Layer Perceptron, SLP)***\n",
        "Neurona artificial que realiza una combinación lineal de las entradas y aplica una función de activación para producir la salida.\n",
        "\n",
        "***Perceptrón Multicapa (Multilayer Perceptron, MLP)***\n",
        "Red neuronal con una o más capas ocultas que permite modelar relaciones no lineales.\n",
        "\n",
        "***Capa oculta***\n",
        "Conjunto de neuronas situadas entre la capa de entrada y la de salida; extrae representaciones intermedias.\n",
        "\n",
        "***Función de activación***\n",
        "Función (p. ej., sigmoide, ReLU) que introduce no linealidad en la red.\n",
        "\n",
        "***Exactitud (Accuracy)***\n",
        "Proporción de predicciones correctas sobre el total de ejemplos evaluados.\n",
        "\n",
        "***Precisión (Precision)***\n",
        "Fracción de predicciones positivas que resultan correctas.\n",
        "\n",
        "***Recall (Sensibilidad)***\n",
        "Fracción de ejemplos positivos correctamente identificados.\n",
        "\n",
        "***F1-score***\n",
        "Media armónica de precisión y recall; balancea ambos criterios.\n",
        "\n",
        "***Matriz de confusión***\n",
        "Tabla que resume aciertos y errores de clasificación por clase.\n",
        "\n",
        "***Sobreajuste (Overfitting)***\n",
        "Cuando un modelo se ajusta demasiado a los datos de entrenamiento y pierde capacidad de generalización.\n"
      ],
      "metadata": {
        "id": "u6OYlSuCxi85"
      }
    }
  ]
}